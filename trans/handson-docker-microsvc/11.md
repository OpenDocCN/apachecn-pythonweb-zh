# 处理系统中的更改、依赖项和机密

在本章中，我们将描述与多个微服务交互的不同元素。

我们将研究如何让服务描述其版本的策略，以便相关的微服务能够发现它们，并确保它们已经部署了适当的依赖项。这将允许我们在依赖服务中定义部署顺序，如果不是所有依赖项都准备好了，则将停止服务的部署。

本章介绍如何定义集群范围内的配置参数，以便使用 Kubernetes ConfigMap 在多个微服务之间共享这些参数，并在单个位置进行管理。我们还将学习如何处理配置参数，这些参数是团队中大多数人都无法访问的机密，如加密密钥。

本章将介绍以下主题：

*   了解跨微服务的共享配置
*   处理库伯内特斯的秘密
*   定义影响多个服务的新功能
*   处理服务依赖关系

在本章结束时，您将了解如何为安全部署准备依赖服务，以及如何在微服务中包含在其预期部署之外无法访问的秘密。

# 技术要求

该代码可在 GitHub 上的以下 URL 获取：[https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter11](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter11) 。请注意，该代码是`Chapter10`中代码的扩展，包含本章所述的额外元素。该结构与代码名为`microservices`的子目录和 Kubernetes 配置文件名为`kubernetes`的子目录相同。

要安装群集，您需要使用以下命令构建每个单独的微服务：

```
$ cd Chapter11/microservices/
$ cd rsyslog
$ docker-compose build
...
$ cd frontend
$ ./build-test.sh
...
$ cd thoughts_backend
$./build-test.sh
...
$ cd users_backend
$ ./build-test.sh
... 
```

这将构建所需的服务。

Note that we use the `build-test.sh` script. We will explain how it works in this chapter.

然后，创建`namespace`示例，使用`Chapter11/kubernetes`子目录中的配置启动 Kubernetes 集群：

```
$ cd Chapter11/kubernetes
$ kubectl create namespace example
$ kubectl apply --recursive -f .
...
```

这会将微服务部署到集群。

The code included in `Chapter11` has some issues and **won't** deploy correctly until it is fixed. This is the expected behavior. During the chapter, we will explain the two problems: the secrets not getting configured, and the dependency for Frontend not getting fulfilled, stopping it from starting.

Keep reading the chapter to find the problems described. The solution is proposed as an assessment.

为了能够访问不同的服务，您需要更新您的`/etc/hosts`文件以包含以下行：

```
127.0.0.1 thoughts.example.local
127.0.0.1 users.example.local
127.0.0.1 frontend.example.local
```

这样，您就可以访问本章的服务。

# 了解跨微服务的共享配置

某些配置可能是多个微服务所共有的。在我们的示例中，我们正在为数据库连接复制相同的值。我们可以使用 ConfigMap 并在不同的部署中共享它，而不是重复每个部署文件上的值。

We've seen how to add ConfigMap to include files in [Chapter 10](10.html), *Monitoring Logs and Metrics*, under the *Setting up metrics* section. It was used for a single service, though.

ConfigMap 是一组键/值元素。它们可以作为环境变量或文件添加。在下一节中，我们将添加一个包含集群中所有共享变量的常规配置文件。

# 添加 ConfigMap 文件

`configuration.yaml`文件包含系统的通用配置。可在`Chapter11/kubernetes`子目录中找到：

```
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: shared-config
  namespace: example
data:
  DATABASE_ENGINE: POSTGRES
  POSTGRES_USER: postgres
  POSTGRES_HOST: "127.0.0.1"
  POSTGRES_PORT: "5432"
  THOUGHTS_BACKEND_URL: http://thoughts-service
  USER_BACKEND_URL: http://users-service
```

与数据库相关的变量`DATABASE_ENGINE`、`POSTGRES_USER`、`POSTGRES_HOST`和`POSTGRES_PORT`在 Thinks 后端和用户后端共享。

The `POSTGRES_PASSWORD` variable is a secret. We will describe this later in this chapter in the *Handling Kubernetes secrets* section.

前端服务中使用了`THOUGHTS_BACKEND_URL`和`USER_BACKEND_URL`变量。不过，它们在集群中很常见。任何想要连接到 Thinks 后端的服务都应该使用与`THOUGHTS_BACKEND_URL`中所述相同的 URL。

尽管到目前为止，它只在单个服务（前端）中使用，但它符合系统范围内的变量描述，应该包含在常规配置中。

One of the advantages of having a shared repository for variables is to consolidate them.

While creating multiple services and developing them independently, it is quite common to end up using the same information, but in two slightly different ways. Teams developing independently won't be able to share information perfectly, and this kind of mismatch will happen.

For example, one service can describe an endpoint as `URL=http://service/api`, and another service using the same endpoint will describe it as `HOST=service PATH=/api`. The code of each service handles the configuration differently, though they connect to the same endpoint. This makes it more difficult to change the endpoint in a unified way, as it needs to be changed in two or more places, in two ways.

A shared place is a good way to first detect these problems, as they normally go undetected if each service keeps its own independent configuration, and then to adapt the services to use the same variable, reducing the complexity of the configuration.

我们示例中 ConfigMap 的名称是元数据中定义的`shared-config`，与任何其他 Kubernetes 对象一样，它可以通过`kubectl`命令进行管理。

# 使用 kubectl 命令

ConfigMap 信息可以通过一组常用的`kubectl`命令进行检查。这使我们能够发现群集中已定义的 ConfigMap 实例：

```
$ kubectl get configmap -n example shared-config
NAME               DATA AGE
shared-config      6    46m
```

注意 ConfigMap 包含的键或变量的数量是如何显示的；这里是`6`。要查看 ConfigMap 的内容，请使用`describe`：

```
$ kubectl describe configmap -n example shared-config
Name: shared-config
Namespace: example
Labels: <none>
Annotations: kubectl.kubernetes.io/last-applied-configuration:
 {"apiVersion":"v1","data":{"DATABASE_ENGINE":"POSTGRES","POSTGRES_HOST":"127.0.0.1","POSTGRES_PORT":"5432","POSTGRES_USER":"postgres","THO...

Data
====
POSTGRES_HOST:
----
127.0.0.1
POSTGRES_PORT:
----
5432
POSTGRES_USER:
----
postgres
THOUGHTS_BACKEND_URL:
----
http://thoughts-service
USER_BACKEND_URL:
----
http://users-service
DATABASE_ENGINE:
----
POSTGRES
```

如果您需要更改 ConfigMap，您可以使用`kubectl edit`命令，或者更好的是，更改`configuration.yaml`文件并使用以下命令重新应用它：

```
$ kubectl apply -f kubernetes/configuration.yaml
```

这将覆盖所有值。

The configuration won't be applied automatically to the Kubernetes cluster. You'll need to redeploy the pods affected by the changes. The easiest way is to delete the affected pods and allow the deployment to recreate them.

On the other hand, if Flux is configured, it will redeploy the dependent pods automatically. Keep in mind that a change in ConfigMap (referenced in all pods) will trigger a redeploy on all pods in that situation.

现在，我们将了解如何将 ConfigMap 添加到部署中。

# 将 ConfigMap 添加到部署中

一旦 ConfigMap 就位，就可以使用它与不同的部署共享其变量，从而维护一个中心位置来更改变量并避免重复。

让我们看看微服务的每个部署（Thinks 后端、Users 后端和前端）是如何使用`shared-config`ConfigMap 的。

# 对后端配置映射配置的思考

后端部署定义如下：

```
spec:
    containers:
        - name: thoughts-backend-service
          image: thoughts_server:v1.5
          imagePullPolicy: Never
          ports:
              - containerPort: 8000
          envFrom:
              - configMapRef:
                    name: shared-config
          env:
              - name: POSTGRES_DB
                value: thoughts
          ...
```

完整的`shared-config`配置图将被注入 pod。请注意，这包括以前在 pod 中不可用的`THOUGHTS_BACKEND_URL`和`USER_BACKEND_URL`环境变量。可以添加更多的环境变量。在这里，我们离开了`POSTGRES_DB`而不是将其添加到 ConfigMap。

我们可以在 pod 中使用`exec`进行确认。

Note that to be able to connect the secret, it should be properly configured. Refer to the *Handling Kubernetes secrets* section.

要检查容器内部，检索 pod 名称并在其中使用`exec`，如下命令所示：

```
$ kubectl get pods -n example
NAME                              READY STATUS  RESTARTS AGE
thoughts-backend-5c8484d74d-ql8hv 2/2   Running 0        17m
...
$ kubectl exec -it thoughts-backend-5c8484d74d-ql8hv -n example /bin/sh
Defaulting container name to thoughts-backend-service.
/opt/code $ env | grep POSTGRES
DATABASE_ENGINE=POSTGRESQL
POSTGRES_HOST=127.0.0.1
POSTGRES_USER=postgres
POSTGRES_PORT=5432
POSTGRES_DB=thoughts
/opt/code $ env | grep URL
THOUGHTS_BACKEND_URL=http://thoughts-service
USER_BACKEND_URL=http://users-service
```

`env`命令返回所有的环境变量，但是有很多环境变量是由 Kubernetes 自动添加的。

# 用户后端配置映射配置

用户后端配置与我们刚才看到的前一种配置类似：

```
spec:
    containers:
        - name: users-backend-service
          image: users_server:v2.3
          imagePullPolicy: Never
          ports:
              - containerPort: 8000
          envFrom:
              - configMapRef:
                    name: shared-config
          env:
              - name: POSTGRES_DB
                value: thoughts
          ...
```

`POSTGRES_DB`的值与 Thinks 后端中的值相同，但我们将其留在这里是为了说明如何添加更多环境变量。

# 前端配置映射配置

前端配置仅使用 ConfigMap，因为不需要额外的环境变量：

```
spec:
    containers:
        - name: frontend-service
          image: thoughts_frontend:v3.7
          imagePullPolicy: Never
          ports:
              - containerPort: 8000
          envFrom:
              - configMapRef:
                    name: shared-config
```

前端播客现在还将包含与数据库连接的信息，这是它不需要的。这对于大多数配置参数都很好。

You can also use multiple ConfigMaps to describe different groups of configurations, if necessary. It is simpler to handle them in a big bucket with all the configuration parameters, though. This will help to catch duplicated parameters and ensure that you have all the required parameters in all microservices.

但是，必须更加小心地处理一些配置参数，因为它们很敏感。例如，我们在`shared-config`配置映射中省略了`POSTGRES_PASSWORD`变量。这使我们能够登录到数据库中，并且不应将其存储在任何带有其他参数的文件中，以避免意外暴露。

为了处理这类信息，我们可以使用库伯尼特斯的秘密。

# 处理库伯内特斯的秘密

秘密是一种特殊的配置。需要保护它们不被其他使用它们的微服务读取。它们通常是敏感数据，如私钥、加密密钥和密码。

请记住，读取机密是有效的操作。毕竟，它们需要被使用。机密与其他配置参数的区别在于，它们需要受到保护，以便只有授权的源才能读取它们。

秘密应该由环境注入。这要求代码能够检索配置机密，并为当前环境使用正确的配置机密。它还避免了将秘密存储在代码中。

Remember *never* to commit production secrets in your Git repositories. The Git tree means that, even if it's deleted, the secret is retrievable. This includes the GitOps environment.

Also, use different secrets for different environments. The production secrets require more care than the ones in test environments.

在我们的 Kubernetes 配置中，授权来源是使用它们的微服务，以及通过`kubectl`访问的系统管理员。

让我们看看如何管理这些秘密。

# 在库伯内特斯存储秘密

Kubernetes 将秘密作为一种特殊的配置映射值来处理。它们可以在系统中定义，然后在相同的环境中应用，例如 ConfigMap。与常规 ConfigMaps 的区别在于，信息在内部受到保护。虽然可以通过`kubectl`访问它们，但可以防止意外接触。

可以通过`kubectl`命令在集群中创建秘密。它们不应该通过文件和 GitOps 或 Flux 创建，而应该手动创建。这避免了在 GitOps repo 下存储机密。

需要秘密操作的 POD 将在其部署文件中指明。这在 GitOps 源代码控制下是安全的，因为它不存储秘密，只存储对秘密的引用。部署 pod 后，它将使用适当的引用并解码机密。

Logging into the pod will grant you access to the secret. This is normal, since, inside the pod, the application needs to read its value. Granting access to execute commands in the pod will grant them access to the secrets inside, so keep it in mind. You can read Kubernetes documentation about the best practices of the secrets to understand and adjust depending on your requirements ([https://kubernetes.io/docs/concepts/configuration/secret/#best-practices](https://kubernetes.io/docs/concepts/configuration/secret/#best-practices)).

现在我们知道了如何处理它们，让我们看看如何创造这些秘密。

# 创造秘密

让我们来创造库伯内特斯的秘密。我们将保存以下秘密：

*   PostgreSQL 密码
*   用于签名和验证请求的公钥和私钥

我们将它们存储在同一个 Kubernetes 秘密中，该秘密可以有多个密钥。以下命令显示如何生成一对密钥：

```
$ openssl genrsa -out private_key.pem 2048
Generating RSA private key, 2048 bit long modulus
........+++
.................+++
e is 65537 (0x10001)
$ openssl rsa -in private_key.pem -outform PEM -pubout -out public_key.pub
writing RSA key
$ ls 
private_key.pem public_key.pub
```

这些钥匙对你来说是独一无二的。我们将使用它们替换前面章节中存储的示例键。

# 在集群中存储秘密

将机密存储在集群中，在`thoughts-secrets`机密下。记住将其存储在`example`名称空间中：

```
$ kubectl create secret generic thoughts-secrets --from-literal=postgres-password=somepassword --from-file=private_key.pem --from-file=public_key.pub -n example
```

您可以在命名空间中列出机密：

```
$ kubectl get secrets -n example
NAME             TYPE   DATA AGE
thoughts-secrets Opaque 3    41s
```

您可以描述这些秘密以获取更多信息：

```
$ kubectl describe secret thoughts-secrets -n example
Name: thoughts-secrets
Namespace: default
Labels: <none>
Annotations: <none>

Type: Opaque

Data
====
postgres-password: 12 bytes
private_key.pem: 1831 bytes
public_key.pub: 408 bytes
```

您可以获取机密的内容，但检索到的数据以 Base64 编码。

Base64 is an encoding scheme that allows you to transform binary data into text and vice versa. It is widely used. This allows you to store any binary secret, not only text. It also means that the secrets are not displayed in plain text when retrieved, adding a small layer of protection in cases such as unintentional display in screens.

要获取该秘密，请使用通常的`kubectl get`命令，如下所示。我们使用`base64`命令对其进行解码：

```
$ kubectl get secret thoughts-secrets -o yaml -n example
apiVersion: v1
data:
 postgres-password: c29tZXBhc3N3b3Jk
 private_key.pem: ...
 public_key.pub: ...
$ echo c29tZXBhc3N3b3Jk | base64 --decode
somepassword
```

同样，如果您编辑一个秘密来更新它，那么输入应该用 Base64 编码。

# 秘密部署配置

我们需要在部署配置中配置秘密用法，以便在所需的 pod 中提供秘密。例如，在用户后端`deployment.yaml`配置文件中，我们有以下代码：

```
spec:
    containers:
    - name: users-backend-service
      ...
      env:
      ...
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            name: thoughts-secrets
            key: postgres-password
        volumeMounts:
        - name: sign-keys
          mountPath: "/opt/keys/"

    volumes:
    - name: sign-keys
      secret:
        secretName: thoughts-secrets
        items:
        - key: public_key.pub
          path: public_key.pub
        - key: private_key.pem
          path: private_key.pem
```

我们创建直接来自秘密的`POSTGRES_PASSWORD`环境变量。我们还创建了一个名为`sign-keys`的卷，其中包含两个键作为文件`public_key.pub`和`private_key.pem`。它安装在`/opt/keys/`路径中。

同样，Thinks 后端的`deployment.yaml`文件包含机密，但只有 PostgreSQL 密码和`public_key.pub`。请注意，私钥没有添加，因为 Thinks 后端不需要私钥，而且它不可用。

对于前端，只需要公钥。现在，让我们确定如何检索这些秘密。

# 通过应用程序检索机密

对于`POSTGRES_PASSWORD`环境变量，我们不需要更改任何内容。它已经是一个环境变量，代码正在从中提取它。

但是对于存储为文件的秘密，我们需要从适当的位置检索它们。存储为文件的机密是签名身份验证头的密钥。所有的微服务都需要公共文件，而私钥只在用户后端需要。

现在，让我们来看一看用户后端的 OutT0A.文件：

```
import os
PRIVATE_KEY = ...
PUBLIC_KEY = ...

PUBLIC_KEY_PATH = '/opt/keys/public_key.pub'
PRIVATE_KEY_PATH = '/opt/keys/private_key.pem'

if os.path.isfile(PUBLIC_KEY_PATH):
    with open(PUBLIC_KEY_PATH) as fp:
        PUBLIC_KEY = fp.read()

if os.path.isfile(PRIVATE_KEY_PATH):
    with open(PRIVATE_KEY_PATH) as fp:
        PRIVATE_KEY = fp.read()
```

当前键仍作为默认值存在。当未装入机密文件时，它们将用于单元测试。

It is worth saying it again, but please *do not* use any of these keys. These are for running tests only and available to anyone that has access to this book.

如果`/opt/keys/`路径中的文件存在，则将读取这些文件，并将内容存储在适当的常量中。用户后端需要公钥和私钥。

在 Thinks Backend`config.py` 文件中，我们只检索公钥，如下代码所示：

```
import os
PUBLIC_KEY = ...

PUBLIC_KEY_PATH = '/opt/keys/public_key.pub'

if os.path.isfile(PUBLIC_KEY_PATH):
    with open(PUBLIC_KEY_PATH) as fp:
        PUBLIC_KEY = fp.read()
```

前端服务在`settings.py`文件中添加公钥：

```
TOKENS_PUBLIC_KEY = ...

PUBLIC_KEY_PATH = '/opt/keys/public_key.pub'

if os.path.isfile(PUBLIC_KEY_PATH):
    with open(PUBLIC_KEY_PATH) as fp:
        TOKENS_PUBLIC_KEY = fp.read()
```

此配置使机密可用于应用程序，并关闭机密值的循环。现在，microservices 集群使用来自秘密值的签名密钥，这是存储敏感数据的安全方法。

# 定义影响多个服务的新功能

我们讨论了单个微服务领域内的变更请求。但是，如果我们需要部署一个在两个或多个微服务中工作的功能，该怎么办？

与整体式方法相比，这些类型的功能应该相对较少，并且是微服务开销的主要原因之一。在一块巨石中，这种情况根本不可能发生，因为一切都包含在巨石的墙壁内。

同时，在微服务体系结构中，这是一个复杂的变化。这涉及到驻留在两个不同回购协议中的每个相关微服务上的至少两个独立功能。回购协议很可能由两个不同的团队开发，或者至少由不同的人负责每个功能。

# 一次部署一个更改

为了确保这些特性可以一次一个地顺利部署，它们需要保持向后兼容性。这意味着，当部署了服务 A 而不是服务 B 时，您需要能够处于中间阶段。微服务中的每个更改都需要尽可能小以最小化风险，并且应该一次引入一个更改。

我们为什么不同时部署它们呢？因为同时发布两个微服务是危险的。首先，部署不是即时的，因此过时的服务有时会发送或接收系统不准备处理的呼叫。这将产生可能影响客户的错误。

但有可能出现一种情况，其中一个微服务不正确，需要回滚。然后，系统处于不一致的状态。依赖的微服务也需要回滚。这本身是有问题的，但在调试此问题的过程中，如果两个微服务都被卡住，并且在问题得到解决之前无法更新，则会使情况变得更糟。

在健康的微服务环境中，部署将非常频繁。因为另一个服务需要工作而不得不停止微服务的管道是一个糟糕的处境，这只会增加压力和紧迫性。

Remember that we talked about the speed of deployment and change. Deploying small increments often is the best way to ensure that each deployment will be of high quality. The constant flow of incremental work is very important.

Interrupting this flow due to an error is bad, but the effect multiplies quickly if the inability to deploy affects the pace of multiple microservices.

同时部署多个服务也可能会造成死锁，这两个服务都需要工作来修复这种情况。这使得开发和解决问题的时间变得复杂。

需要进行分析以确定哪些微服务依赖于其他微服务，而不是同时部署。大多数时候，这是显而易见的。在我们的示例中，前端依赖于 Thinkings 后端，因此涉及到它们的任何更改都需要从 Thinkings 后端开始，然后移动到前端。

Actually, the Users Backend is a dependency of both, so assuming there's a change that affects the three of them, you'll need to first change the Users Backend, then the Thoughts Backend, and finally the Frontend.

请记住，有时，部署可能需要跨服务移动多次。例如，假设我们对身份验证头的签名机制进行了更改。然后，流程应如下所示：

1.  在用户后端实现新的身份验证系统，但通过配置更改继续使用旧系统生成令牌。到目前为止，集群中仍然使用旧的身份验证过程。
2.  更改 Thinks 后端以允许同时使用旧的和新的身份验证系统。请注意，它尚未激活。
3.  将前端更改为同时使用两种身份验证系统。不过，在这一点上，新系统尚未使用。
4.  更改用户后端中的配置以生成新的身份验证令牌。现在是新系统开始使用的时候了。在部署过程中，可能会生成一些旧的系统令牌。
5.  用户后端和前端将使用系统中的任何令牌，无论是新的还是旧的。旧代币将随着时间的推移而消失，因为它们将过期。新令牌是唯一被创建的令牌。
6.  作为可选阶段，可以从系统中删除旧的身份验证系统。这三个系统可以在没有任何依赖关系的情况下删除它们，因为此时不使用系统。

在流程的任何步骤中，服务都不会中断。每个零钱都是安全的。这个过程正在缓慢地使整个系统进化，但如果出现问题，每个单独的步骤都是可逆的，并且服务不会中断。

系统倾向于通过添加新特性来开发，并且很少有清理阶段。通常情况下，系统会在很长一段时间内使用不推荐的功能，即使该功能没有在任何地方使用。

We will talk a bit more about clean-up in [Chapter 12](12.html), *Collaborating and Communicating across Teams*.

配置更改也可能需要此过程。在该示例中，更改签名身份验证标头所需的私钥将需要以下步骤：

1.  使 Thinks 后端和前端能够处理多个公钥。这是一个先决条件，也是一个新特性。
2.  在 Thinks 后端中更改已处理的密钥，使其同时具有旧公钥和新公钥。到目前为止，没有使用新密钥签名的头在系统中流动。
3.  更改前端中的已处理关键点，使其同时具有旧关键点和新关键点。但是，没有使用新密钥签名的头在系统中流动。
4.  更改用户后端的配置以使用新私钥。从现在起，系统中会有使用新私钥签名的头。其他微服务也能处理这些问题。
5.  系统仍然接受使用旧密钥签名的头。等待一段安全时间，以确保所有旧标头都已过期。
6.  删除用户后端中旧密钥的配置。

可以每隔几个月重复步骤 2 至 6 以使用新钥匙。

此过程称为**密钥旋转**，它被认为是一种良好的安全实践，因为它缩短了密钥有效时的使用寿命，缩短了系统易受密钥泄漏影响的时间窗口。为了简单起见，我们没有在示例系统中实现它，但这样做只是一个推荐的练习。尝试更改示例代码以实现此关键点旋转示例！

完整的系统功能可能涉及多个服务和团队。为了帮助协调系统的依赖关系，我们需要知道服务的某个依赖关系何时部署并准备就绪。我们将在[第 12 章](12.html)*中讨论团队间的沟通，*中讨论团队间的合作和沟通，但我们可以通过使服务 API 明确描述部署了哪个版本的服务来帮助编程，我们将在*中讨论服务依赖关系*部分

如果刚刚部署的新版本出现问题，可以通过回滚快速恢复部署。

# 回滚微服务

回滚是将一个微服务快速后退到以前版本的过程。

当刚刚发布的新版本中出现灾难性错误时，可以触发此过程，因此可以快速解决此问题。考虑到该版本目前已经兼容，这可以在很短的反应时间内自信地完成。通过 GitOps 原则，可以通过`revert`提交来恢复旧版本。

The `git revert` command allows you to create a commit that undoes another, applying the same changes in reverse.

This is a quick way to undo a particular change, and to allow later to *revert the revert* and reintroduce the changes. You can check the Git documentation for more details ([https://git-scm.com/docs/git-revert](https://git-scm.com/docs/git-revert)[).](https://git-scm.com/docs/git-revert)

考虑到继续前进的战略方针，回滚是一项临时措施，在实施过程中，将阻止微服务中的新部署。应该尽快创建一个解决导致灾难性部署的 bug 的新版本，以保持正常的发布流程。

随着部署的频率越来越高，检查也越来越到位，回滚将越来越不常见。

# 处理服务依赖关系

为了允许服务检查其依赖项是否具有正确的版本，我们将使服务通过 RESTful 端点公开其版本。

我们将遵循 GitHub 中 Thinks 后端的示例，网址为：[https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter11/microservices/thoughts_backend](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter11/microservices/thoughts_backend) 。

检查前端是否有版本（[https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter11/microservices/frontend](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter11/microservices/frontend) ）。

流程的第一步是为每个服务正确定义版本。

# 对服务进行版本控制

为了清楚地了解软件的进展，我们需要命名要部署的不同版本。当我们使用`git`跟踪更改时，系统中的每个提交都有一个单独的提交 ID，但它不遵循任何特定的模式。

为了给它一个意义并对它们进行排序，我们需要开发一个版本模式。制作版本模式有多种方法，包括按发布日期（Ubuntu 使用此方法）或`major.minor.patch`。

在任何地方使用相同的版本控制方案都有助于开发团队间的通用语言和理解。它还可以帮助管理层了解发布时间以及变化速度方面的变化。与您的团队商定在您的组织中有意义的版本控制方案，并在所有服务中遵循该方案。

对于本例，我们将使用一个`vMajor.Minor`模式，用户后端的版本为`v2.3`。

软件版本控制中最常见的模式是语义版本控制。此版本控制模式对于包和面向客户的 API 很有用，但对于内部微服务 API 不太有用。让我们看看它的特点是什么。

# 语义版本控制

语义版本控制对每个不同版本号的更改施加意义。这使得了解版本之间的更改范围以及在依赖系统上进行更新是否有风险变得容易。

语义版本定义每个版本有三个数字：主要、次要和补丁，通常描述为`major.minor.patch`*。*

增加这些数字中的任何一个都具有特定的含义，如下所示：

*   增加主数字会产生向后不兼容的更改。
*   增加次要数量会添加新功能，但会保持向后兼容性。
*   增加补丁数量可以修复 bug，但不会添加任何新功能。

例如，Python 在此模式下工作如下：

*   Python3 包含了与 Python2 的兼容性更改。
*   与 Python3.6 相比，Python3.7 版引入了新功能。
*   与 Python3.7.3 相比，Python3.7.4 增加了安全性和 bug 修复。

此版本控制方案在与外部合作伙伴通信时非常有用，对于大型发行版和标准软件包非常有用。但是对于微服务中的小增量更改，它不是很有用。

正如我们在前几章中所讨论的，持续集成的关键是进行非常小的更改。它们不应该破坏向后兼容性，但随着时间的推移，旧的功能将被删除。每个微服务以受控的方式与其他服务协同工作。与外部软件包相比，没有必要有如此强大的功能标签。服务的使用者是集群中受到严格控制的其他微服务。

Some projects are abandoning semantic versioning due to this change in operation. For example, the Linux kernel stopped using semantic versioning to produce new versions without any particular meaning ([http://lkml.iu.edu/hypermail/linux/kernel/1804.1/06654.html](http://lkml.iu.edu/hypermail/linux/kernel/1804.1/06654.html)), as changes from one version to the next are relatively small.

Python will also treat version 4.0 as *the version that goes after 3.9*, without major changes like Python 3 had ([http://www.curiousefficiency.org/posts/2014/08/python-4000.html](http://www.curiousefficiency.org/posts/2014/08/python-4000.html)).

这就是为什么在内部，*不推荐*语义版本控制。保持一个类似的版本控制方案可能很有用，但不必强迫它进行兼容性更改，只需不断增加数量，而不需要对何时更改次要或主要版本做出具体要求。

不过，从外部来看，版本号可能仍然具有营销意义。对于外部可访问的端点，使用语义版本控制可能很有趣。

一旦确定了服务的版本，我们就可以在公开此信息的端点上工作。

# 添加版本终结点

要部署的版本可以从 Kubernetes 部署或 GitOps 配置中读取。但有一个问题。有些配置可能会产生误导，或者不是唯一指向单个图像的配置。例如，`latest`标记可能在不同的时间代表不同的容器，因为它被覆盖。

此外，还存在访问 Kubernetes 配置或 GitOps repo 的问题。对于开发人员来说，这种配置可能是可用的，但它们不适用于微服务（也不应该）。

要允许集群中其余的微服务发现服务的版本，最好的方法是在 RESTful API 中显式创建版本端点。服务版本的发现是被授予的，因为它使用的接口与它将在任何其他请求中使用的接口相同。让我们看看如何实现它。

# 获取版本

要提供该版本，我们首先需要将其记录到服务中。

如前所述，版本存储为 Git 标记。这将是我们版本中的佳能。我们还将添加提交的 gitsha-1 以避免任何差异。

The SHA-1 is a unique ID that identifies each commit. It's produced by hashing the Git tree, so that it's able to capture any change—either the content or the tree history. We will use the full SHA-1 of 40 characters, even though sometimes it is abbreviated to eight or less.

可以使用以下命令获得 commit SHA-1：

```
$ git log --format=format:%H -n 1
```

这将打印最后的提交信息，并且仅打印带有`%H`描述符的 SHA。

要获取此提交引用的标记，我们将使用`git-describe`命令：

```
$ git describe --tags
```

基本上，`git-describe`找到与当前提交最近的标记。如果此提交由标记进行标记（对于我们的部署来说应该是这样），那么它将返回标记本身。如果不是，则在标记后面加上关于提交的额外信息，直到它到达当前提交为止。下面的代码显示了如何使用`git describe`，具体取决于提交的代码版本。请注意，与标记不关联的代码如何返回最近的标记和额外数字：

```
$ # in master branch, 17 commits from the tag v2.3
$ git describe
v2.3-17-g2257f9c
$ # go to the tag
$ git checkout v2.3
$ git describe
v2.3
```

这总是返回一个版本，并允许我们一目了然地检查当前提交中的代码是否标记在`git`中。

Anything that gets deployed to an environment should be tagged. Local development is a different matter, as it consists of code that is not ready yet.

我们可以以编程方式存储这两个值，允许我们自动执行并将它们包含在 Docker 映像中。

# 在映像中存储版本

我们希望在图像中有可用的版本。因为映像是不可变的，所以在构建过程中这样做是目标。我们需要克服的限制是 Dockerfile 进程不允许我们在主机上执行命令，只能在容器内执行。我们需要在构建时将这些值注入 Docker 映像。

A possible alternative is to install Git inside the container, copy the whole Git tree, and obtain the values. This is usually discouraged because installing Git and the full source tree adds a lot of space to the container, something that is worse. During the build process, we already have Git available, so we just need to be sure to inject it externally, which is easy to do with a build script.

传递值的最简单方法是通过`ARG`参数。作为构建过程的一部分，我们将把它们转换为环境变量，这样它们就可以像配置的任何其他部分一样容易地使用。让我们看看下面代码中的 DoCKFILE：

```
# Prepare the version
ARG VERSION_SHA="BAD VERSION"
ARG VERSION_NAME="BAD VERSION"
ENV VERSION_SHA $VERSION_SHA
ENV VERSION_NAME $VERSION_NAME
```

我们接受一个`ARG`参数，然后通过`ENV`参数将其转换为环境变量。为了简单起见，两者都有相同的名称。`ARG`参数具有拐角情况的默认值。

这使得在我们使用`build.sh`脚本构建版本后，版本可用（在容器内），该脚本获取值并调用`docker-compose`以版本作为参数进行构建，使用以下步骤：

```
# Obtain the SHA and VERSION
VERSION_SHA=`git log --format=format:%H -n 1`
VERSION_NAME=`git describe --tags`
# Build using docker-compose with arguments
docker-compose build --build-arg VERSION_NAME=${VERSION_NAME} --build-arg VERSION_SHA=${VERSION_SHA}
# Tag the resulting image with the version
docker tag thoughts_server:latest throughs_server:${VERSION_NAME}
```

在构建过程之后，该版本可以作为容器中的标准环境变量使用。

We included a script (`build-test.sh` ) in each of the microservices in this chapter (for example, [https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter11/microservices/thoughts_backend/build-test.sh](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter11/microservices/thoughts_backend/build-test.sh)). This mocks the SHA-1 and version name to create a synthetic version for tests. It sets up the `v2.3` version for the Users Backend and `v1.5` for the Thoughts Backend. These will be used for examples in our code.

Check that the Kubernetes deployments include those versions (for example, the [https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter11/microservices/thoughts_backend/docker-compose.yaml#L21](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter11/microservices/thoughts_backend/docker-compose.yaml#L21) image is the `v1.5` version).

此外，`VERSION_NAME`还可以作为脚本的参数来自 CI 管道。为此，您需要替换脚本以从外部接受它，如`build-ci.sh`脚本中所示：

```
#!/bin/bash
if [ -z "$1" ]
  then
    # Error, not version name
    echo "No VERSION_NAME supplied"
    exit -1
fi

VERSION_SHA=`git log --format=format:%H -n 1`
VERSION_NAME=$1

docker-compose build --build-arg VERSION_NAME=${VERSION_NAME} --build-arg VERSION_SHA=${VERSION_SHA}
docker tag thoughts_server:latest throughs_server:${VERSION_NAME}
```

这些脚本的所有版本都包含以`VERSION_NAME`作为标记的图像标记。

我们可以在 Python 代码中使用容器内的版本检索环境变量，并在端点中返回它们，从而使该版本易于通过外部 API 访问。

# 实现版本端点

在`admin_namespace.py`文件中，我们将使用以下代码创建一个新的`Version`端点：

```
import os

@admin_namespace.route('/version/')
class Version(Resource):

    @admin_namespace.doc('get_version')
    def get(self):
        '''
        Return the version of the application
        '''
        data = {
            'commit': os.environ['VERSION_SHA'],
            'version': os.environ['VERSION_NAME'],
        }

        return data
```

好的，现在这个代码非常简单。它使用`os.environ`检索构建过程中注入的环境变量作为配置参数，并返回一个带有 commit SHA-1 和标记（描述为版本）的字典。

该服务可以使用`docker-compose`在本地构建和运行。要测试对`/admin/version`中端点的访问并进行检查，请执行以下步骤：

```
$ cd Chapter11/microservices/thoughts_backend
$ ./build.sh
...
Successfully tagged thoughts_server:latest
$ docker-compose up -d server
Creating network "thoughts_backend_default" with the default driver
Creating thoughts_backend_db_1 ... done
Creating thoughts_backend_server_1 ... done
$ curl http://localhost:8000/admin/version/
{"commit": "2257f9c5a5a3d877f5f22e5416c27e486f507946", "version": "tag-17-g2257f9c"}
```

由于版本可用，我们可以更新自动生成的文档以显示正确的值，如`app.py`所示：

```
import os
...
VERSION = os.environ['VERSION_NAME']
...

def create_app(script=False):
    ...
    api = Api(application, version=VERSION, 
              title='Thoughts Backend API',
              description='A Simple CRUD API')
```

因此，版本会正确地显示在自动招摇过市文档中。一旦微服务的版本可以通过 API 中的端点访问，其他外部服务就可以访问该版本以发现该版本并加以利用。

# 检查版本

能够通过 API 检查版本允许我们以编程方式轻松访问版本。这可以用于多种目的，例如生成一个显示在不同环境中部署的不同版本的仪表板。但我们将探讨引入服务依赖性的可能性。

微服务在启动时可以检查它所依赖的服务，还可以检查它们是否高于预期版本。如果没有，它就不会启动。这避免了在更新依赖关系之前部署一个依赖服务时出现配置问题。这可能发生在复杂系统中，在部署中没有很好的协调。

为了检查版本，在`start_server.sh`中启动服务器时，我们将首先调用一个小脚本来检查依赖关系。如果不可用，它将产生错误并停止。我们将检查前端是否有可用的 Think 后端版本或更高版本。

我们将在示例中调用的脚本被称为`check_dependencies_services.py`，它在`start_server.sh`中被前端调用。

`check_dependencies_services`脚本可分为三部分：所需依赖项的列表；检查一个依赖项；以及检查每个依赖项的主要部分。让我们来看看这三个部分。

# 所需版本

第一部分描述了每个依赖项以及所需的最低版本。在我们的示例中，我们规定`thoughts_backend`必须是`v1.6`版本或更高版本：

```
import os

VERSIONS = {
    'thoughts_backend': 
        (f'{os.environ["THOUGHTS_BACKEND_URL"]}/admin/version',
         'v1.6'),
}
```

这将重用环境变量`THOUGHTS_BACKEND_URL`，并使用特定的版本路径完成 URL。

主要部分介绍了所描述的所有依赖项，以检查它们。

# 主要功能

主函数遍历`VERSIONS`字典，并针对每个字典执行以下操作：

*   调用端点
*   解析结果并获取版本
*   打电话`check_version`查看是否正确

如果失败，则以`-1`状态结束，因此脚本报告为失败。这些步骤通过以下代码执行：

```
import requests

def main():
    for service, (url, min_version) in VERSIONS.items():
        print(f'Checking minimum version for {service}')
        resp = requests.get(url)
        if resp.status_code != 200:
            print(f'Error connecting to {url}: {resp}')
            exit(-1)

        result = resp.json()
        version = result['version']
        print(f'Minimum {min_version}, found {version}')
        if not check_version(min_version, version):
            msg = (f'Version {version} is '
                    'incorrect (min {min_version})')
            print(msg)
            exit(-1)

if __name__ == '__main__':
    main()
```

主功能还打印一些消息，以帮助理解不同的阶段。为了调用版本端点，它使用`requests`包，并期望有`200`状态代码和可解析的 JSON 结果。

Note that this code iterates through the `VERSION` dictionary. So far, we only added one dependency, but the User Backend is another dependency and can be added. It's left as an exercise to do.

版本字段将在`check_version`函数中检查，我们将在下一节中看到。

# 检查版本

`check_version`功能检查返回的当前版本是否高于或等于最低版本。为了简化它，我们将使用`natsort`包对版本进行排序，然后检查最低版本。

You can check out the `natsort` full documentation ([https://github.com/SethMMorton/natsort](https://github.com/SethMMorton/natsort)). It can sort a lot of natural strings and can be used in a lot of situations.

基本上，`natsort` 支持对常见的版本控制模式进行排序，这包括我们前面描述的标准版本控制模式（`v1.6`高于`v1.5`。以下代码使用库对两个版本进行排序，并验证最低版本是否为较低版本：

```
from natsort import natsorted

def check_version(min_version, version):
    versions = natsorted([min_version, version])
    # Return the lower is the minimum version
    return versions[0] == min_version
```

有了这个脚本，我们现在可以启动服务，它将检查 Thinks 后端是否有正确的版本。如果您按照*技术要求*部分所述启动服务，您会看到前端启动不正常，并产生`CrashLoopBackOff`状态，如下所示：

```
$ kubectl get pods -n example
NAME READY STATUS RESTARTS AGE
frontend-54fdfd565b-gcgtt 0/1 CrashLoopBackOff 1 12s
frontend-7489cccfcc-v2cz7 0/1 CrashLoopBackOff 3 72s
grafana-546f55d48c-wgwt5 1/1 Running 2 80s
prometheus-6dd4d5c74f-g9d47 1/1 Running 2 81s
syslog-76fcd6bdcc-zrx65 2/2 Running 4 80s
thoughts-backend-6dc47f5cd8-2xxdp 2/2 Running 0 80s
users-backend-7c64564765-dkfww 2/2 Running 0 81s
```

使用`kubectl logs`命令检查其中一个前端吊舱的日志，查看原因，如下所示：

```
$ kubectl logs frontend-54fdfd565b-kzn99 -n example
Checking minimum version for thoughts_backend
Minimum v1.6, found v1.5
Version v1.5 is incorrect (min v1.6)
```

要解决此问题，您需要使用更高版本构建 Thinks 后端的版本，或者降低依赖性要求。这将作为本章末尾的评估。

# 总结

在本章中，我们学习了如何处理同时使用多个微服务的元素。

首先，我们讨论了当新功能需要更改多个微服务时应遵循的策略，包括如何以有序的方式部署小增量，以及如何在出现灾难性问题时回滚。

然后，我们讨论了定义一个清晰的版本控制模式，并向 RESTful 接口添加一个版本端点，以允许为微服务自发现版本。这种自我发现可用于确保在不存在依赖关系的情况下不部署依赖于另一个的微服务，这有助于协调发布。

The code in GitHub for the Frontend in this chapter ([https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter11/microservices/frontend](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter11/microservices/frontend)) includes a dependency to the Thoughts Backend that will stop deploying it. Note that the code, as is, won't work. Fixing it is left as an exercise.

我们还学习了如何使用 ConfigMap 来描述 Kubernetes 集群中跨不同服务共享的配置信息。我们后来描述了如何使用 Kubernetes secrets 来处理敏感且需要特别小心的配置。

在下一章中，我们将看到以高效方式协调使用不同微服务的不同团队的各种技术。

# 问题

1.  在微服务体系结构系统中发布更改与在整体中发布更改有什么区别？
2.  为什么在微服务架构中发布的更改应该很小？
3.  语义版本控制是如何工作的？
4.  在微服务体系结构系统中，与内部接口的语义版本控制相关的问题有哪些？
5.  添加版本端点的优点是什么？
6.  我们如何解决本章代码中的依赖性问题？
7.  我们应该将哪些配置变量存储在共享的 ConfigMap 中？
8.  您能否描述在单个共享配置映射中获取所有配置变量的优缺点？
9.  Kubernetes 配置图和 Kubernetes 秘密有什么区别？
10.  我们怎样才能改变库伯内特的秘密？
11.  想象一下，基于配置，我们决定将`public_key.pub`文件从机密更改为 ConfigMap。我们必须实施哪些变革？

# 进一步阅读

为了在 AWS 上处理您的秘密，您可以与名为 CredStash（[的工具进行交互 https://github.com/fugue/credstash](https://github.com/fugue/credstash) ）。您可以在书*AWS SysOps Cookbook–第二版*（[中了解有关如何使用它的更多信息 https://www.packtpub.com/cloud-networking/aws-administration-cookbook-second-edition](https://www.packtpub.com/cloud-networking/aws-administration-cookbook-second-edition) ）。